{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is tokenization?\n",
        "\n",
        "We usually start an NLP project with a large body of text, called a corpus. This could be a collection of tweets, website reviews or transcriptions of films, for example. We need to pre-process our corpus to give it enough structure to be used in a machine learning model and tokenization is the most common first step.\n",
        "\n",
        "Tokenization is the process of breaking down a corpus into tokens. The procedure might look like segmenting a piece of text into sentences and then further segmenting these sentences into individual words, numbers and punctuation, which would be tokens."
      ],
      "metadata": {
        "id": "TCc9JT5uQM8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization using spaCy"
      ],
      "metadata": {
        "id": "JbepFWijQrO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7ec5n1PZ6L",
        "outputId": "f00e38d7-6bdc-492c-8252-057447dc0fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spacy 3.7.6\n"
          ]
        }
      ],
      "source": [
        "#Import spacy\n",
        "import spacy\n",
        "print(spacy.__name__,spacy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the spacy model\n",
        "nlp=spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "bNHn2z5YQ2h-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize string\n",
        "s=\"Noah doesn't like to run when its rains.\"\n",
        "doc=nlp(s)"
      ],
      "metadata": {
        "id": "1dSBcnWxRLvI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print tokens\n",
        "print([t.text for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNf013KGRZRC",
        "outputId": "649f9786-4705-4970-d30e-39a6631ccdc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Noah', 'does', \"n't\", 'like', 'to', 'run', 'when', 'its', 'rains', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The doc object is a container, which can be indexed and sliced like a list.\n",
        "\n"
      ],
      "metadata": {
        "id": "dGlhmE4XRrJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Index and slice example\n",
        "print(doc[0])\n",
        "print(doc[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiY6cJXLRiDJ",
        "outputId": "7a142319-03e0-4a83-a46a-14f69862b258"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noah\n",
            "Noah doesn't\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Object types\n",
        "print(type(doc))\n",
        "print(type(doc[0]))\n",
        "print(type(doc[0:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKuC6C40R1HP",
        "outputId": "51bfed9c-9168-4ed5-fbd2-934a4c68df76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "<class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Token attribute examples\n",
        "print(doc[3].text)\n",
        "print(doc[3].lang_)\n",
        "print(doc[3].__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIb8XQBJSIJS",
        "outputId": "417d62da-6b36-4c35-95cb-80a69596a87a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "like\n",
            "en\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Locate index of tokens\n",
        "print([(t.text,t.i) for t in doc[:6]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pXmp95MSh9z",
        "outputId": "60a4c27d-1e89-4021-c893-ee71c4ddf4c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Noah', 0), ('does', 1), (\"n't\", 2), ('like', 3), ('to', 4), ('run', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize multiple sentences\n",
        "s=\"Hello there! General Kenobi.You are a bold one.\"\n",
        "doc=nlp(s)"
      ],
      "metadata": {
        "id": "vE311Fa2Sxks"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print sentences\n",
        "list(doc.sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33vVV4uTGFO",
        "outputId": "0ec84e90-3070-41fb-fedd-a1287da0b838"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Hello there!, General Kenobi., You are a bold one.]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#object type\n",
        "type(list(doc.sents)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii_xWU9PTL7u",
        "outputId": "a5694eb4-0e8b-4f62-fe57-761c7e54ba1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print tokens\n",
        "print([t.text for t in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A64-UsWETVzP",
        "outputId": "9d647eb7-2c43-4e34-cf3f-699603e91d83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'there', '!', 'General', 'Kenobi', '.', 'You', 'are', 'a', 'bold', 'one', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCF8HiqtTfql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}